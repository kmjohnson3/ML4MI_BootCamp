{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQfkzl7ZyVIO"
   },
   "source": [
    "This is an MRI based reconstruction demo, for 2D MRI data. The network is relatively similar to the AutoMap technique (https://arxiv.org/abs/1704.08841). This is a relatively 'brute force' aproach to image reconstruction in which the transoform is given no direct knowledge of the physics (although the network architecture is a bit tuned to the problem). In this work, we are assuming one direction is fully sampled (i.e. frequency encoded).\n",
    "\n",
    "# MRI Sampling\n",
    "In MRI the data is often discretely Fourier transoformed in one direction leading to the discretized signal model:\n",
    "\n",
    "$s(k)=\\sum_{j=1}^{N}\\rho (x_j)e^{i2\\pi kx}$\n",
    "\n",
    "The expected reconstruction for fully sampled data is an inverse discrete Fourier transform: \n",
    "\n",
    "$s(x)=\\sum_{j=1}^{N}s(k_j)e^{i2\\pi k_j x}$\n",
    "\n",
    "# Questions to think about:\n",
    "1) What is the minimal network architecture to compute a DFT? It's a square matrix multiply.\n",
    "\n",
    "2) What is the apropriate loss function if we wish to train an image reconstruction? \n",
    "\n",
    "3) What is the role of the convolutional layers? When are they needed?\n",
    "\n",
    "4) What is the network learning if you train on simulated images?\n",
    "\n",
    "# PyTorch\n",
    "This exercise is written in PyTorch (https://pytorch.org/docs/stable/index.html). There is also a Keras version available online. PyTorch has some different coding constructs but there are similar ideas to Keras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CRqgrJE2Ta5"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JEeJpTYyyVIQ",
    "outputId": "da02f9f8-cc59-4005-b143-81c67c642e8b"
   },
   "outputs": [],
   "source": [
    "''' \n",
    "In python you need to import libraries in order to use them. \n",
    "'''\n",
    "\n",
    "# Using PyTorch for this code\n",
    "import torch\n",
    "  \n",
    "# Utilities\n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "''' \n",
    "We will define some functions to use later. Normally you might put these in another file and import them\n",
    "'''\n",
    "\n",
    "# Some support functions\n",
    "def montage( img_in, size=(3,5) ):\n",
    "    for j in range(size[0]):\n",
    "        plot_image = img_in[0+size[1]*j,:,:]\n",
    "        for i in range(size[1]-1):\n",
    "            plot_image = np.concatenate((plot_image, img_in[1+i+size[1]*j,:,:]), axis=1)\n",
    "        \n",
    "        if j == 0:\n",
    "            img = plot_image\n",
    "        else:\n",
    "            img = np.concatenate((img,plot_image),axis=0)\n",
    "    return img\n",
    "  \n",
    "def complex_to_channels( img_in):\n",
    "  return(np.stack(img_in.real,img_in.imag))\n",
    "  \n",
    "def channels_to_complex( img_in):\n",
    "  return(img_in[...,0]+1j*img_in[...,1])\n",
    "\n",
    "\n",
    "'''\n",
    "Mount your google drive, we'll grab data from the shared folder\n",
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vi9ablPpyVIT"
   },
   "source": [
    "# Training Images\n",
    "We are training this network using a simulation enviroment. Images are grabbed from a set of MRI brain images. This example is using the output dicoms which have been preprocessed. We then simulate the image to MRI raw data conversion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZFxAwy-rUsm",
    "outputId": "019da629-27d2-4726-d5f0-22f06ef09e52"
   },
   "outputs": [],
   "source": [
    "# load training, validation, and testing data\n",
    "import h5py\n",
    "with h5py.File('/content/drive/MyDrive/ML4MI_BOOTCAMP_DATA/ImageReconstructionData.h5','r') as hf:\n",
    "  x_train = np.array(hf['x_train'])\n",
    "  x_val = np.array(hf['x_val'])\n",
    "\n",
    "print(f'Validate Dataset Size {x_val.shape}')\n",
    "print(f'Train Dataset Size {x_train.shape}')\n",
    "N = x_train.shape[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdaPO4_syVIa"
   },
   "source": [
    "# Simulate Sampling\n",
    "MRI data generation is aproximately dsicrete sampling of a continous Fourier transform the the data. In this example, we are using a Discrete Fourier transform to aproximate this. We also consider the case when we randomly remove data points. This would allow us to go faster and is used in compressed sensing application ( e.g. https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.21391 ). Noise is added a complex, white, gaussian noise (MRI noise is so called Johnson/Nyquist noise). Things to try:\n",
    "\n",
    "1) Add higher levels of noise. What happens to the training rate and output images? \n",
    "\n",
    "2) Increase the undersampling rate. How does the neural network compare to traditional aproaches? \n",
    "\n",
    "3) Comment the FFT shift, does the network still learn the transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XdM9OkvyVIc",
    "outputId": "10a28e3e-ad8c-4a32-bfd1-151e15957dcc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The creates a sampling mask which can be used to subsample the data.\n",
    "'''\n",
    "\n",
    "# Get the number of phase encodes\n",
    "undersample_factor = 1.5 \n",
    "noise_level = 0.001\n",
    "\n",
    "number_phase_encodes = int(N/undersample_factor)\n",
    "print('Using ' + str(number_phase_encodes) + ' phase encode')\n",
    "\n",
    "# Create a random mask to resample the data\n",
    "idx = np.full(N, False)\n",
    "idx[:number_phase_encodes] = True\n",
    "np.random.seed(1) # Keep this one so code is reproducible\n",
    "np.random.shuffle(idx)\n",
    "sampling_mask = idx\n",
    "\n",
    "\n",
    "'''\n",
    "Fourier transform, subsample, and add noise (Train Data)\n",
    "'''\n",
    "Nexamples = x_train.shape[0]\n",
    "kspace_train = np.zeros((Nexamples,N,number_phase_encodes,2),x_train.dtype)\n",
    "for example in range(x_train.shape[0]):\n",
    "  \n",
    "  if example % 1000 == 0:\n",
    "    print(f'Working on example {example} of {Nexamples}')\n",
    "      \n",
    "  # Grab one image\n",
    "  temp = x_train[example,:,:,0] + 1j*x_train[example,:,:,1]\n",
    "  \n",
    "  # Fourier Transform\n",
    "  kspace_temp = np.fft.fftn(temp,axes=(1,))/N\n",
    "  kspace_temp = np.fft.fftshift(kspace_temp,axes=(1,))\n",
    "  kspace_temp =np.stack( (kspace_temp.real, kspace_temp.imag), axis=-1)\n",
    "  \n",
    "  # Subsample\n",
    "  kspace_temp = kspace_temp[:,sampling_mask,:]\n",
    "  \n",
    "  # Add noise\n",
    "  kspace_temp += noise_level*np.random.randn(*kspace_temp.shape)\n",
    "\n",
    "  # Put back\n",
    "  kspace_train[example,:,:,:] = kspace_temp\n",
    "  \n",
    "print('Dimensions of training data are ' + str(kspace_train.shape) + '[ Examples x Nx x Ny x Channels]')\n",
    "\n",
    "\n",
    "'''\n",
    "Fourier transform, subsample, and add noise (Validate Data) \n",
    "'''\n",
    "Nexamples = x_val.shape[0]\n",
    "kspace_val = np.zeros((Nexamples,N,number_phase_encodes,2),x_train.dtype)\n",
    "for example in range(x_val.shape[0]):\n",
    "  \n",
    "  if example % 1000 == 0:\n",
    "    print(f'Working on example {example} of {Nexamples}')\n",
    "      \n",
    "  # Grab one image\n",
    "  temp = x_val[example,:,:,0] + 1j*x_val[example,:,:,1]\n",
    "  \n",
    "  # Fourier Transform\n",
    "  kspace_temp = np.fft.fftn(temp,axes=(1,))/N\n",
    "  kspace_temp = np.fft.fftshift(kspace_temp,axes=(1,))\n",
    "  kspace_temp =np.stack( (kspace_temp.real, kspace_temp.imag), axis=-1)\n",
    "  \n",
    "  # Subsample\n",
    "  kspace_temp = kspace_temp[:,sampling_mask,:]\n",
    "  \n",
    "  # Add noise\n",
    "  kspace_temp += noise_level*np.random.randn(*kspace_temp.shape)\n",
    "\n",
    "  # Put back\n",
    "  kspace_val[example,:,:,:] = kspace_temp\n",
    "  \n",
    "print('Dimensions of validation data are ' + str(kspace_val.shape) + '[ Examples x Nx x Ny x Channels]')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "rypEBBdfyVIl",
    "outputId": "51345b49-535a-41e4-eb54-40f541d9ba8f"
   },
   "outputs": [],
   "source": [
    "example = 400\n",
    "\n",
    "# Show one image and k-space pair\n",
    "img = x_train[example,:,:,0] + 1j*x_train[example,:,:,1]\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.abs(img),cmap='gray')\n",
    "plt.title('Grayscale')\n",
    "\n",
    "img = kspace_train[example,:,:,0] + 1j*kspace_train[example,:,:,1]\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(np.abs(img),cmap='gray')\n",
    "plt.title('K-Space (1D FFT)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYe5C1eayVIq"
   },
   "source": [
    "# Build the network architecture\n",
    "\n",
    "In PyTorch, we need to create a model just as we do in Keras. However, models are usually defined as a class. A class is a structure that holds data and functions. We base our classes on torch.nn.Module and will only modify the __init__ and forward functions. The __init__ module will define the modules (convultions, activations) that we will use in the forward pass. This method is very flexible and PyTorch's autograd functionally will generally handle all the backpropagation. \n",
    "\n",
    "**FYI** PyTorch defaults to channels first. So the images will be [batch index, channels, x, y]\n",
    "\n",
    "We will define the following functions.\n",
    "\n",
    "*   FullyConnected a modules to perform the fully connected layers\n",
    "*   Denoiser a module to perform denoising using convolutions\n",
    "*   AutoMap a simple module which sequentially applies FullyConnected and Denoiser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEykndHfyVIr",
    "outputId": "83ebee8b-8762-41df-db89-c3245ffce9a4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FullyConnected(torch.nn.Module):\n",
    "  \"\"\"A fully connected network to transform from k-space to images.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_phase_encodes, image_size):\n",
    "    super(FullyConnected, self).__init__()\n",
    "    self.input_phase_encodes = input_phase_encodes\n",
    "    self.image_size = image_size\n",
    "\n",
    "    # These two convolution will be fully connected along one dimension. We are not\n",
    "    # using a fully connected network in 2D due to memory contraints and the fact \n",
    "    # that the data is undersampling in one dimension. \n",
    "    self.conv1 = nn.Conv2d(in_channels=2, out_channels=2*image_size, kernel_size=(1, input_phase_encodes), padding='valid')\n",
    "    self.conv2 = nn.Conv2d(in_channels=2, out_channels=2*image_size, kernel_size=(1, image_size), padding='valid')\n",
    "\n",
    "  def forward(self, data):\n",
    "\n",
    "    # This is a fully connected network followed by reshaping into an image\n",
    "    layer = self.conv1(data)\n",
    "    layer = layer.view((-1,2,self.image_size,self.image_size))\n",
    "    layer = torch.moveaxis(layer,-2,-1)\n",
    "\n",
    "    # This is a fully connected network followed by reshaping into an image\n",
    "    layer = self.conv2(layer)\n",
    "    layer = layer.view((-1,2,self.image_size,self.image_size))\n",
    "    layer = torch.moveaxis(layer,-2,-1)\n",
    "\n",
    "    return layer\n",
    "\n",
    "class Denoiser(torch.nn.Module):\n",
    "  \"\"\"An image to image denoiser\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, depth=3, initial_features=8):\n",
    "    super(Denoiser, self).__init__()\n",
    "\n",
    "    # The channels start as complex (2 channel) and we can increase them during the denoiser\n",
    "    in_channels = 2\n",
    "    out_channels = initial_features\n",
    "\n",
    "    # This will build the list of operators, similar to keras sequential which each being a convolution followed by activation\n",
    "    layers = []\n",
    "    for l in range(depth):\n",
    "      layers.append( nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3,3), padding='same'))\n",
    "      in_channels = out_channels\n",
    "      layers.append(torch.nn.ReLU())\n",
    "\n",
    "    # Last layer is a convolution without activiation\n",
    "    layers.append( nn.Conv2d(in_channels=in_channels, out_channels=2, kernel_size=(3,3), padding='same'))\n",
    "    \n",
    "    # This stores the layers in a list that will be tracked for backpropogation\n",
    "    self.convolutions = nn.ModuleList(layers)\n",
    "    \n",
    "  def forward(self, image):\n",
    "    for l in self.convolutions:\n",
    "      image = l(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "class AutoMap(torch.nn.Module):\n",
    "  \"\"\"A network combining the fully connected network with the image denoiser.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_phase_encodes, image_size, depth=3):\n",
    "    super(AutoMap, self).__init__()\n",
    "\n",
    "    self.fc_net = FullyConnected( input_phase_encodes, image_size)\n",
    "    self.denoiser = Denoiser(depth)\n",
    "    \n",
    "  def forward(self, data):\n",
    "    image = self.fc_net(data)\n",
    "    image = self.denoiser(image)\n",
    "\n",
    "    return image\n",
    "   \n",
    "\n",
    "# To run the model we need to create an object based on those class definitions above\n",
    "recon_model = AutoMap(input_phase_encodes=number_phase_encodes, image_size=N)\n",
    "\n",
    "# Models have a device they run on. We need to put the model on the gpu\n",
    "recon_model = recon_model.cuda()\n",
    "\n",
    "# Torch summary enables similar formatting to Keras\n",
    "from torchsummary import summary\n",
    "summary(recon_model, (2,120,120))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7cJZC0ta2ox"
   },
   "source": [
    "# Data Loader\n",
    "In PyTorch, it is very useful to have data as a torch Dataset. This could select all the images in a folder, generate data on the fly, etc. In this case, we will write a loader which just grabs data. Following this, we define a data loader which handles batching, shuffling, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsqjB28lxhU0"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  \n",
    "  def __init__(self, kspace_data, image_data):\n",
    "        self.kspace_data = np.moveaxis( kspace_data, -1, 1)\n",
    "        self.image_data = np.moveaxis( image_data, -1, 1)\n",
    "\n",
    "  def __len__(self):\n",
    "        return self.kspace_data.shape[0]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "        return self.kspace_data[idx], self.image_data[idx]\n",
    "\n",
    "# Create datasets\n",
    "dataset_train = Dataset( kspace_train, x_train)\n",
    "dataset_val = Dataset( kspace_val, x_val)\n",
    "\n",
    "# Create data loader to handle shuffling, batching, etc\n",
    "train_generator = torch.utils.data.DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "val_generator = torch.utils.data.DataLoader(dataset_val, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdjic3fSbnGD"
   },
   "source": [
    "# Training\n",
    "Training in PyTorch is a bit more transparent than in Keras. The below will train the model with the key steps being:\n",
    "\n",
    "\n",
    "\n",
    "*   Define an optimizer which will optimize the paramaters we pass to it\n",
    "*   Define a loss function. This could be anything written in PyTorch but we will use a predefined function for mean squared error\n",
    "*   Loop over the data with a for loop.\n",
    "\n",
    "For each batch, we will:\n",
    "* Zero the gradients (gradients accumulate over multiple passes)\n",
    "* Perform a forward pass\n",
    "* Calculate the loss\n",
    "* Perform a backwards pass\n",
    "* Increment the weights using optimizer.step() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "3BKNIemY0D4i",
    "outputId": "8a898431-cdf9-4818-f073-0e74b5e9aefa"
   },
   "outputs": [],
   "source": [
    "# Define an optimizer \n",
    "optimizer = torch.optim.Adam( recon_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define a loss function\n",
    "loss_fcn = nn.MSELoss()\n",
    "\n",
    "# Get a device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Ensure the model weights are on the desired device\n",
    "recon_model.to(device)\n",
    "\n",
    "# Define number of epochs\n",
    "n_epochs = 20\n",
    "\n",
    "# Empty list to store losses over epochs\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Epoch loop\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "  # Put the model in train mode\n",
    "  recon_model.train()\n",
    "\n",
    "  # Loop over training batches\n",
    "  train_loss_avg = 0.0\n",
    "  for kspace, image in train_generator:\n",
    "    \n",
    "    # Move data to the GPU\n",
    "    image = image.to(device)\n",
    "    kspace = kspace.to(device)\n",
    "\n",
    "    # Zero out the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    image_guess = recon_model(kspace)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fcn( image_guess, image)\n",
    "\n",
    "    # Store loss \n",
    "    train_loss_avg += loss.detach()\n",
    "\n",
    "    # Backwards with perform back propogation to get weights\n",
    "    loss.backward()\n",
    "    \n",
    "    # Take a gradient descent step\n",
    "    optimizer.step()\n",
    "\n",
    "  # Store the average loss\n",
    "  train_loss_avg /= len(train_generator)\n",
    "  train_losses.append(train_loss_avg)\n",
    "\n",
    "  # Switch to eval mode (weights fixed)\n",
    "  recon_model.eval()\n",
    "\n",
    "  # Loop eval batches\n",
    "  val_loss_avg = 0.0\n",
    "  for count, (kspace, image) in enumerate(val_generator):\n",
    "\n",
    "    # Move data to the GPU\n",
    "    image = image.to(device)\n",
    "    kspace = kspace.to(device)\n",
    "   \n",
    "    # Forward pass\n",
    "    image_guess = recon_model(kspace)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fcn( image_guess, image)\n",
    "    val_loss_avg += loss.detach()\n",
    "\n",
    "    # Plotting images\n",
    "    if count == 0:\n",
    "      image_mag = torch.abs(image[0,0,...] + 1j*image[0,1,...])\n",
    "      image_guess_mag = torch.abs(image_guess[0,0,...] + 1j*image_guess[0,1,...])\n",
    "\n",
    "      clear_output(wait=True)\n",
    "      plt.figure(figsize=(10,3))\n",
    "      plt.subplot(131)\n",
    "      plt.imshow(image_mag.detach().cpu().numpy(),cmap='gray')\n",
    "      plt.title('Truth')\n",
    "      plt.subplot(132)\n",
    "      plt.imshow(image_guess_mag.detach().cpu().numpy(),cmap='gray')\n",
    "      plt.title('NN Guess')\n",
    "\n",
    "  # Store the average loss\n",
    "  val_loss_avg /= len(val_generator)\n",
    "  val_losses.append(val_loss_avg)\n",
    "\n",
    "  # Plotting loss curve\n",
    "  plt.subplot(133)\n",
    "  plt.semilogy(train_losses, label=\"Loss\")\n",
    "  plt.semilogy(val_losses, label=\"Loss (validation)\")\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  print(f'Epoch = {epoch} Loss = {train_loss_avg}, Val Loss = {val_loss_avg}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNWmO6ZLyVI0"
   },
   "source": [
    "# Run the model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "FItk3W_byVI4",
    "outputId": "10aa6357-64c6-449f-dc1d-a9c3f530683f"
   },
   "outputs": [],
   "source": [
    "example = 400\n",
    "\n",
    "# Test with synthetic data\n",
    "kspace = kspace_val[example,...]\n",
    "kspace = np.expand_dims(kspace,0)\n",
    "\n",
    "image = x_val[example,...]\n",
    "image = np.expand_dims(image,0)\n",
    "\n",
    "# Get the prediction\n",
    "kspace_torch = torch.tensor(np.moveaxis( kspace, -1, 1))\n",
    "predicted_image = recon_model(kspace_torch.to(device))\n",
    "predicted_image = predicted_image.detach().cpu().numpy()\n",
    "predicted_image = np.moveaxis(predicted_image,1,-1)\n",
    "\n",
    "# Convert to complex\n",
    "predicted_image = np.squeeze(channels_to_complex(predicted_image))\n",
    "act_image =  np.squeeze(channels_to_complex(x_val[example,...]))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(11, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(131)\n",
    "plt.imshow(np.abs(predicted_image),cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.title('Predicted')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(np.abs(act_image),cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.title('True Image')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(np.abs(act_image-predicted_image),cmap='gray',vmin=0)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.title('Difference Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-txDIz5lyVI9"
   },
   "source": [
    "# Compare to least squares solution with data\n",
    "Here we compare to an alterantive aproach, regularied least squares. In this technique, we build an encoding matrix which simulates the data acquisition. Then we minimize:\n",
    "\n",
    "$\\parallel Ex-d \\parallel_2 +  \\lambda \\parallel x \\parallel_2$\n",
    "\n",
    "Where $\\lambda$ is a factor that regularizes the solution when its illposed ( see https://en.wikipedia.org/wiki/Tikhonov_regularization ). The solution to this set of equations is:\n",
    "\n",
    "$ \\widetilde{x} = (E^hE + \\lambda I)^{-1}E^hd$\n",
    "\n",
    "Where I is an identity matrix. Similar to the neural network this is an aproximate solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 681
    },
    "id": "sfsr-k_-yVI_",
    "outputId": "5cbac55f-9941-41a2-f15d-e0d934608fcd"
   },
   "outputs": [],
   "source": [
    "# Lets also solve this a different way using a matrix inverse\n",
    "def DFT_matrix(N):\n",
    "    i, j = np.meshgrid(np.arange(N), np.arange(N))\n",
    "    omega = np.exp( 2 * math.pi * 1J / N )\n",
    "    W = np.power( omega, i * j ) / N #math.sqrt(N)\n",
    "    return W\n",
    "\n",
    "E = DFT_matrix(N)\n",
    "E = np.fft.fftshift(E,axes=(0,))\n",
    "E = E[idx,:]\n",
    "\n",
    "# Grab the data\n",
    "D = np.matrix.getH(channels_to_complex(kspace_val[400,...]))\n",
    "\n",
    "# Solve for psuedo inverse\n",
    "Eh = np.matrix.getH(E)\n",
    "EhE = np.matmul(Eh,E)\n",
    "Ei = np.linalg.inv(EhE + 0.000001*np.identity(N))\n",
    "EiEh = np.matmul(Ei,Eh)\n",
    "\n",
    "linear_algebra_prediction = np.transpose(np.matmul(EiEh,D))\n",
    "    \n",
    "plt.figure(figsize=(11, 11), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.imshow(np.abs(linear_algebra_prediction),cmap='gray',vmin=0)\n",
    "plt.axis('off')\n",
    "plt.title('Least Squares Solution')\n",
    "plt.subplot(234)\n",
    "plt.imshow(np.abs(linear_algebra_prediction-act_image),cmap='gray',vmin=0,vmax=0.2)\n",
    "plt.axis('off')\n",
    "plt.title('Difference Least Squares')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.imshow(np.abs(predicted_image),cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "plt.title('Neural Net Prediction')\n",
    "plt.subplot(235)\n",
    "plt.imshow(np.abs(predicted_image-act_image),cmap='gray',vmin=0,vmax=0.2)\n",
    "plt.axis('off')\n",
    "plt.title('Difference Neural Net')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.imshow(np.abs(act_image),cmap='gray',vmin=0,vmax=1)\n",
    "plt.axis('off')\n",
    "plt.title('Actual Image')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Image Domain Mean Squared Error NN = ' + str(np.sum(np.square(abs(np.squeeze(predicted_image) - act_image)))) )\n",
    "print('Image Domain Mean Squared Error LS = ' + str(np.sum(np.square(abs(linear_algebra_prediction - act_image)))) )\n",
    "\n",
    "# Lets also get the kspace error\n",
    "kspace_NN = np.matmul(E,np.squeeze(predicted_image))\n",
    "kspace_LA = np.matmul(E,linear_algebra_prediction)\n",
    "\n",
    "# Difference \n",
    "diff_kspace_NN = kspace_NN - D\n",
    "diff_kspace_LA = kspace_LA - D\n",
    "print('Kspace Mean Squared Error NN = ' + str(np.sum(np.square(abs(diff_kspace_NN)))) )\n",
    "print('Kspace Mean Squared Error LS = ' + str(np.sum(np.square(abs(diff_kspace_LA)))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bErWlC1oyVJD"
   },
   "source": [
    "# Load real MRI data to test\n",
    "This is actual acquired MRI data from a brain scan consisting. The data size is larger and we crop in k-space. Just to make things doable in a short time we are keeping everything 1D, as above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Wsb0va1yVJE",
    "outputId": "984b5254-ecdf-43ba-945f-9888855c2dfe"
   },
   "outputs": [],
   "source": [
    "# Load a Kspace dataset from an actual acquisition\n",
    "with h5py.File('/content/drive/MyDrive/ML4MI_BOOTCAMP_DATA/Example_MRI_Data.h5','r') as hf:\n",
    "    kspace_mri = np.array(hf['Kspace'])\n",
    "\n",
    "#Crop Kspace\n",
    "crop = ( kspace_mri.shape[-2] - N ) // 2\n",
    "kspace_mri = kspace_mri[...,::2,crop:-crop]\n",
    "    \n",
    "print(f'Kspace size = {kspace_mri.shape} [ channels, slices, Nx, Ny], type = {kspace_mri.dtype}')\n",
    "coils = kspace_mri.shape[0]\n",
    "slices = kspace_mri.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDpA4fbOyVJI"
   },
   "source": [
    "# Run a traditional reconstruction \n",
    "The most common reconstruction on MRI scanners is to just do a discrete Fourier transform of the data. Just a note, the data actually has 48 recievers of the signal. We are taking the sum of squares to average these signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "id": "Ek5-9k5dyVJJ",
    "outputId": "3c6e88c3-4423-4ac3-9aac-95e0afd6f792"
   },
   "outputs": [],
   "source": [
    "# Traditional recon of fully sampled data\n",
    "image_full = np.fft.ifftn(kspace_mri,axes=(-1,))\n",
    "\n",
    "# do sum of squares to average coils (detectors)\n",
    "image_full = np.sum(abs(image_full),axis=0)\n",
    "image_full = np.sqrt(image_full)\n",
    "\n",
    "# Make a montage (there are other options)\n",
    "plot_image = montage(image_full[8::2,:,:])  \n",
    "    \n",
    "# Show the image\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(plot_image,aspect=1,interpolation='bilinear',cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('DFT of Kspace')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zFZ9MqfyVJL"
   },
   "source": [
    "# Do inference on the real MRI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWezUcd-9zC1"
   },
   "source": [
    "Machine Learning Based Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    },
    "id": "JNKqr0LOyVJM",
    "outputId": "c54bf911-9a50-4385-8c5b-ccb9bba5c390"
   },
   "outputs": [],
   "source": [
    "# Subsample kspace and convert to channels\n",
    "kspace_mri2 = kspace_mri[:,:,:,sampling_mask]\n",
    "kspace_mri2 = np.stack((kspace_mri2.real,kspace_mri2.imag),axis=-1)\n",
    "\n",
    "\n",
    "kspace_mri2 = np.reshape(kspace_mri2,(-1,N,number_phase_encodes,2))\n",
    "print(kspace_mri2.shape)\n",
    "\n",
    "# Run model\n",
    "kspace_torch = torch.tensor(np.moveaxis( kspace_mri2, -1, 1))\n",
    "predicted_image = recon_model(kspace_torch.to(device))\n",
    "predicted_image = predicted_image.detach().cpu().numpy()\n",
    "image_NN = np.moveaxis(predicted_image,1,-1)\n",
    "print(image_NN.shape)\n",
    "\n",
    "# Reshape\n",
    "image_NN = np.reshape( image_NN,(coils,slices,N,N,2))\n",
    "image_NN = channels_to_complex(image_NN)\n",
    "\n",
    "# do sum of squares to average coils (detectors)\n",
    "image_NN = np.sum(abs(image_NN),axis=0)\n",
    "image_NN = np.sqrt(image_NN)\n",
    "\n",
    "# Make a montage (there are other options)\n",
    "plot_image = montage( image_NN[8::2,:,:])\n",
    "\n",
    "# Show the image\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(plot_image,aspect=1,interpolation='bilinear',cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Neural network prediction from Kspace')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Af-0f8ds9n_m"
   },
   "source": [
    "Linear algebra based solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "id": "DB9r2kdbyVJQ",
    "outputId": "aa61c5f2-4103-4441-9f97-6c8dd277e89a"
   },
   "outputs": [],
   "source": [
    "image_LA = np.zeros(image_full.shape,dtype=image_full.dtype)\n",
    "\n",
    "for k in range(slices):\n",
    "\n",
    "  # Subsample kspace and convert to channels\n",
    "  kspace_mri2 = np.squeeze(kspace_mri[:,k,:,:])\n",
    "  kspace_mri2 = kspace_mri2[:,:,sampling_mask]\n",
    "\n",
    "  kspace_mri2 = np.reshape(kspace_mri2,(-1,number_phase_encodes))\n",
    "  kspace_mri2 = np.expand_dims(kspace_mri2,-1)\n",
    "  \n",
    "  # Also do for Least squares estimate\n",
    "  image = np.matmul(EiEh,kspace_mri2)\n",
    "  image = np.reshape(image,newshape=(coils,N,N))\n",
    "\n",
    "  # do sum of squares to average coils (detectors)\n",
    "  image = np.sum(abs(image),axis=0)\n",
    "  image = np.sqrt(image)\n",
    "\n",
    "  image_LA[k,:,:] = np.fliplr(image)\n",
    "\n",
    "# Make a montage (there are other options)\n",
    "plot_image = montage( image_LA[8::2,:,:])\n",
    "\n",
    "# Show the image\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(plot_image,aspect=1,interpolation='bilinear',cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Linear algebra prediction from Kspace')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR50C3vkyVJT"
   },
   "source": [
    "# Now compare the solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "vTsV6JDcyVJT",
    "outputId": "d27c9feb-86ce-4611-c8d8-59cb729291c9"
   },
   "outputs": [],
   "source": [
    "slice = 24\n",
    "\n",
    "print(image_LA.shape)\n",
    "print(image_NN.shape)\n",
    "print(image_full.shape)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(131)\n",
    "plt.imshow(abs(image_LA[slice,:,:]),cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Linear Algebra')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(abs(image_NN[slice,:,:]),cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Neural Net')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(abs(image_full[slice,:,:]),cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "liC8He132W2-",
    "outputId": "479bfa9e-d8d8-40bc-a2d3-e531eba9594b"
   },
   "outputs": [],
   "source": [
    "# Slice for viewing\n",
    "slice = 24\n",
    "\n",
    "# Scale to minimize difference (scaling unimportant in MRI)\n",
    "scale_LA = np.sum( image_full*np.conj(image_LA)) /np.sum(image_LA**2)\n",
    "scale_NN = np.sum( image_full*np.conj(image_NN)) /np.sum(image_NN**2)\n",
    "\n",
    "diff_LA = scale_LA*image_LA - image_full\n",
    "diff_NN = scale_NN*image_NN - image_full\n",
    "\n",
    "# Print Error\n",
    "error_LA = np.linalg.norm(diff_LA)/np.linalg.norm(image_full)\n",
    "error_NN = np.linalg.norm(diff_NN)/np.linalg.norm(image_full)\n",
    "\n",
    "print(f'Image MSE Linear Algebra = {error_LA}')\n",
    "print(f'Image MSE Neural Network = {error_NN}')\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(131)\n",
    "plt.imshow(abs(diff_LA[slice,:,:]),cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Linear Algebra')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(abs(diff_NN[slice,:,:]),cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Neural Net')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(abs(image_full[slice,:,:]),cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8P3_AoG-9__n"
   },
   "source": [
    "# Image Recon Challenge\n",
    "Can you fix the image reconstruction example?  The challenge is to reconstruct the images with the following paramaters:\n",
    "\n",
    "undersample_factor = 1.5 \n",
    "noise_level = 0.001; \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CoLab_AutoMap_Recon_Torch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
